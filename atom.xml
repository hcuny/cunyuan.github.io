<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Chosen To Be</title>
  <subtitle>Cunyuan&#39;s Tech Blog</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://hcuny.github.io/"/>
  <updated>2017-05-08T20:38:05.540Z</updated>
  <id>http://hcuny.github.io/</id>
  
  <author>
    <name>Cunyuan(Anthony) Huang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>recommender</title>
    <link href="http://hcuny.github.io/2017/05/08/recommender/"/>
    <id>http://hcuny.github.io/2017/05/08/recommender/</id>
    <published>2017-05-08T17:34:56.000Z</published>
    <updated>2017-05-08T20:38:05.540Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p><strong>Recommendation System</strong> is a facility of predicting user responses to options. Examples:</p>
<ul>
<li>Offering news articles to on-line newspaper readers, based on interest(topics) prediction.</li>
<li>Online retailer suggestions, based on history purchase/search.<a id="more"></a>
Note: physical stores V.S. online institutions, not possible to tailor the store to individual customer, choice governed by overall popularity(sale performance).</li>
</ul>
<h2 id="Content-based-Recommendations"><a href="#Content-based-Recommendations" class="headerlink" title="Content-based Recommendations"></a>Content-based Recommendations</h2><p><strong>Similarity of Items is determined by measuring the similarity in their properties</strong>. Why called content based? Goal is to create both an item profile consisting of feature-value pairs and user profile summarizing the preference of a user.</p>
<h4 id="Item-profile"><a href="#Item-profile" class="headerlink" title="Item profile"></a>Item profile</h4><p>Collection of records representing important characteristics of that item. E.g. stars of a movie, released year, genre… these are features that are tend to be readily available, how about documents? One possible solution is to choose words whose TF-IDF higher than a given threshold to be the feature set.</p>
<h4 id="User-profile"><a href="#User-profile" class="headerlink" title="User profile"></a>User profile</h4><p>Item profile is vectors describing items, user profile is vectors with the same components that describe the user’s preferences–need some aggregation.(naturally take the average)</p>
<p>Eg1(categorical): 20% of the movies that user U likes(0/1) have Julia, profile for U will have 0.2 in component Julia.<br>Eg2(numerical): User U has average rating 3, there’re 3 movies he watched with Julia as a star, having rating 3, 4, 5, then corresponding Score is (0+1+2)/3=1. </p>
<h2 id="Collaborative-Filtering"><a href="#Collaborative-Filtering" class="headerlink" title="Collaborative Filtering"></a>Collaborative Filtering</h2><p><strong>Similarity of Items is determined by the similarity of the ratings of those items by the users who have rated both of them</strong>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Overview&quot;&gt;&lt;a href=&quot;#Overview&quot; class=&quot;headerlink&quot; title=&quot;Overview&quot;&gt;&lt;/a&gt;Overview&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Recommendation System&lt;/strong&gt; is a facility of predicting user responses to options. Examples:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Offering news articles to on-line newspaper readers, based on interest(topics) prediction.&lt;/li&gt;
&lt;li&gt;Online retailer suggestions, based on history purchase/search.
    
    </summary>
    
      <category term="Study Notes" scheme="http://hcuny.github.io/categories/Study-Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://hcuny.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>GMM-HMM-EM</title>
    <link href="http://hcuny.github.io/2017/04/08/GMM-HMM-EM/"/>
    <id>http://hcuny.github.io/2017/04/08/GMM-HMM-EM/</id>
    <published>2017-04-08T19:08:21.000Z</published>
    <updated>2017-04-08T19:47:10.219Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://people.csail.mit.edu/rameshvs/content/gmm-em.pdf" target="_blank" rel="external">GMM</a>, <a href="https://www.cs.cmu.edu/~epxing/Class/10701-08s/recitation/em-hmm.pdf" target="_blank" rel="external">HMM</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://people.csail.mit.edu/rameshvs/content/gmm-em.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GMM&lt;/a&gt;, &lt;a href=&quot;https://www.cs.cmu.ed
    
    </summary>
    
      <category term="Study Notes" scheme="http://hcuny.github.io/categories/Study-Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://hcuny.github.io/tags/Machine-Learning/"/>
    
      <category term="Statistics" scheme="http://hcuny.github.io/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>Dimension Reduction Techniques</title>
    <link href="http://hcuny.github.io/2017/03/21/Dimension-Reduction-Techniques/"/>
    <id>http://hcuny.github.io/2017/03/21/Dimension-Reduction-Techniques/</id>
    <published>2017-03-21T11:24:14.000Z</published>
    <updated>2017-03-21T19:19:51.929Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Curse-of-Dimensionality-different-narrations"><a href="#1-Curse-of-Dimensionality-different-narrations" class="headerlink" title="1. Curse of Dimensionality: different narrations"></a>1. Curse of Dimensionality: different narrations</h2><blockquote>
<p>Similarity function: Almost all pairs of points are equally far away from one another, and almost any two vectors are almost orthogonal.</p>
<p>Statistical Inference: This sparsity is problematic for any method that requires statistical significance. In order to obtain a statistically sound and reliable result, the amount of data needed to support the result often grows exponentially with the dimensionality. Think about the features are supposed to be IDPT in a simple regression problem.</p>
<p>General ML: Given finite number of data samples in a high-dimensional feature space with each feature having a number of possible values, an enormous amount of training data is required to ensure that there are several samples with each combination of values.</p>
</blockquote>
<h2 id="2"><a href="#2" class="headerlink" title="2."></a>2.</h2><p><a href="http://stats.stackexchange.com/questions/135853/lda-vs-svm-for-dimensionality-reduction" target="_blank" rel="external">Link1</a><br><a href="http://stats.stackexchange.com/questions/169436/how-lda-a-classification-technique-also-serves-as-dimensionality-reduction-tec" target="_blank" rel="external">Link2</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Curse-of-Dimensionality-different-narrations&quot;&gt;&lt;a href=&quot;#1-Curse-of-Dimensionality-different-narrations&quot; class=&quot;headerlink&quot; title=&quot;
    
    </summary>
    
      <category term="Study Notes" scheme="http://hcuny.github.io/categories/Study-Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://hcuny.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Summing Up Clustering</title>
    <link href="http://hcuny.github.io/2017/03/16/Summing-Up-Clustering/"/>
    <id>http://hcuny.github.io/2017/03/16/Summing-Up-Clustering/</id>
    <published>2017-03-16T19:22:43.000Z</published>
    <updated>2017-03-20T19:37:44.803Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-Overview"><a href="#1-Overview" class="headerlink" title="1. Overview"></a>1. Overview</h2><p>Clustering algorithm types<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">All types</div><div class="line">│———Prototype Based: point assignment</div><div class="line">│   |   </div><div class="line">|   |———Centroids: distance metric applied to center of a subgroup</div><div class="line">|   |</div><div class="line">|   |———Medoids: representative point of the graph  </div><div class="line">|</div><div class="line">└─── Hierarchical Based</div><div class="line">│   │</div><div class="line">│   │———Agglomeration</div><div class="line">│   │</div><div class="line">│   └───Division</div><div class="line">│   </div><div class="line">└───Density Based</div></pre></td></tr></table></figure></p>
<a id="more"></a>
<h2 id="2-Agglomerative-algorithms"><a href="#2-Agglomerative-algorithms" class="headerlink" title="2. Agglomerative algorithms"></a>2. Agglomerative algorithms</h2><p>Agglomerative algorithms start from singletons, and they differ because of the strategy used to merge 2 clusters, i.e. the metric used to calculate the distance between 2 clusters. Let’s see different strategies of merging clusters.</p>
<ul>
<li>Single Link(MIN): find all min distance between any 2 points from two clusters, and compare. <strong>Min(min(2clusters))</strong><br>Time complexity O(n^2), we do not need to recompute the pairs each time.</li>
<li>Complete Link(MAX/CLIQUE): find all max distance between any 2 points from two clusters. <strong>Min(max(2clusters))</strong><br>Time complexity O(n^2 log n), consider the first merge, compute each pair &amp; sort.</li>
<li>Group Average: keep updating the centroid of each cluster. <strong>Min(d(2centroid))</strong></li>
<li>Ward’s Method: track the increase in squared error when merged. <strong>Min(Increase_SE(2clusters))</strong></li>
</ul>
<p>Real example from the retailer data:<br><img src="/img/aggclustering.png" alt="Alt text" title="Agglomerative Clustering"></p>
<h2 id="3-Divisive-algorithms"><a href="#3-Divisive-algorithms" class="headerlink" title="3. Divisive algorithms"></a>3. Divisive algorithms</h2><p>Interestingly connected with MST, just consider the whole graph made up of weights(distance between 2 clusters), first find the MST, and break link corresponding to the largest distance.</p>
<h2 id="4-K-means-A-better-start"><a href="#4-K-means-A-better-start" class="headerlink" title="4. K-means: A better start"></a>4. K-means: A better start</h2><p>When Initializing k centroids, we’ll not do it randomly, instead:</p>
<blockquote>
<p>Pick one point randomly into the active set s while len(s) &lt;k, add a point whose minimum distance from s is maximized.  </p>
</blockquote>
<p>Then we’ll follow the normal process, assign each point to the nearest centroid &amp; update the centroids. Note that the global minimization function is min(SSE) = sum(sum(SSE each cluster))</p>
<h2 id="5-Find-the-best-k"><a href="#5-Find-the-best-k" class="headerlink" title="5. Find the best k?"></a>5. Find the best k?</h2><p><strong>Elbow Method</strong> – graphically.<br><img src="/img/elbow.png" alt="Alt text" title="Elbow Method"><br>Note: this is typically used in K-means for the case that K is unknown, but in the implementation of the algorithm, we shall assign a K value in advance. (Q: We can also use it in Hierarchical?)<br>For Hierarchical clustering, the process can stop when there’re a fixed number of clusters left, or we can stop until there is no further compact merge, which means we need to define a threshold for the merged diameter/radius.</p>
<h2 id="6-Some-thinking"><a href="#6-Some-thinking" class="headerlink" title="6. Some thinking"></a>6. Some thinking</h2><ul>
<li>Hierarchical clustering is relatively fast because of its greedy nature, the process is very much like constructing a Huffman tree. However, we don’t have a global objective function, which means in each merge decision, it’s not made from a globally optimized point of view, the resulting clusters are not even stable. I think this is the key difference distinguishing it from k-means.</li>
<li><p>Computation complexity for hierarchical clustering:</p>
<blockquote>
<p>In the basic setting, obviously it’s O(n^2)+O((n-1)^2)+O((n-2)^2)… = O(n^3)<br>Now introduce a improved one: initial step: O(n^2), Form the pairs into a priority queue (constant time to find pairs to merge), which takes O(n^2). Here’s the trick: when decide to merge C &amp; D, remove from the priority queue all entries involving C or D (Deletion O(log n), at most 2n deletions). Recompute the pair distance and insert (O(log n) for insertion, and at most n insertions). Thus, overall it’s O(n^2)+O(n^2)+O(n*nlog n)=O(n^2log n).</p>
</blockquote>
</li>
<li><p>Computation complexity for K-means:</p>
<blockquote>
<p>Global optimization function NP Hard. The running time of Lloyd’s algorithm is naively O(nkdi), where n is the number of d-dimensional vectors, k the number of clusters and i the number of iterations needed until convergence.</p>
</blockquote>
</li>
<li><p>Ward’s method can be used as a robust method of Initializing a K-means clustering.</p>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;1-Overview&quot;&gt;&lt;a href=&quot;#1-Overview&quot; class=&quot;headerlink&quot; title=&quot;1. Overview&quot;&gt;&lt;/a&gt;1. Overview&lt;/h2&gt;&lt;p&gt;Clustering algorithm types&lt;br&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;6&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;7&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;8&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;9&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;10&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;11&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;12&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;13&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;14&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;All types&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;│———Prototype Based: point assignment&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;│   |   &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;|   |———Centroids: distance metric applied to center of a subgroup&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;|   |&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;|   |———Medoids: representative point of the graph  &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;|&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;└─── Hierarchical Based&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;│   │&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;│   │———Agglomeration&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;│   │&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;│   └───Division&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;│   &lt;/div&gt;&lt;div class=&quot;line&quot;&gt;└───Density Based&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Study Notes" scheme="http://hcuny.github.io/categories/Study-Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://hcuny.github.io/tags/Machine-Learning/"/>
    
      <category term="Interview" scheme="http://hcuny.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Store Clustering</title>
    <link href="http://hcuny.github.io/2017/03/16/Store-Clustering/"/>
    <id>http://hcuny.github.io/2017/03/16/Store-Clustering/</id>
    <published>2017-03-16T18:44:20.000Z</published>
    <updated>2017-03-16T18:47:32.809Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Clustering-by-geographic-data"><a href="#Clustering-by-geographic-data" class="headerlink" title="Clustering by geographic data"></a>Clustering by geographic data</h3><h3 id="Clustering-directly-by-Sale-performance"><a href="#Clustering-directly-by-Sale-performance" class="headerlink" title="Clustering directly by Sale performance"></a>Clustering directly by Sale performance</h3>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Clustering-by-geographic-data&quot;&gt;&lt;a href=&quot;#Clustering-by-geographic-data&quot; class=&quot;headerlink&quot; title=&quot;Clustering by geographic data&quot;&gt;&lt;/a
    
    </summary>
    
      <category term="Research" scheme="http://hcuny.github.io/categories/Research/"/>
    
      <category term="Thesis" scheme="http://hcuny.github.io/categories/Research/Thesis/"/>
    
    
      <category term="Machine Learning" scheme="http://hcuny.github.io/tags/Machine-Learning/"/>
    
      <category term="Statistics" scheme="http://hcuny.github.io/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>Budget Function</title>
    <link href="http://hcuny.github.io/2017/03/14/budget-function/"/>
    <id>http://hcuny.github.io/2017/03/14/budget-function/</id>
    <published>2017-03-15T00:36:06.000Z</published>
    <updated>2017-03-16T06:12:06.698Z</updated>
    
    <content type="html"><![CDATA[<p>Initially the budget function aims to find the relation between sale data and “labor goal”, my first thought is to do linear regression without any transformation.<br><a id="more"></a><br>First take a look at the scatter plot of all data points.</p>
<p><img src="/img/all_scatter.png" alt="Alt text" title="scatter plot"><br>For fun, plot the overall regression plot for each store, shown in the same graph.</p>
<p><img src="/img/totalplot.png" alt="Alt text" title="total plot"><br>Plot regression forms on each store, note that logically speaking budget function is designed for each store.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sns.lmplot(x=<span class="string">"actual_sales"</span>, y=<span class="string">"labor_goal"</span>, col=<span class="string">"storeid"</span>, data=df_demand, size=<span class="number">3.5</span>,col_wrap=<span class="number">3</span>,aspect=<span class="number">1</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/img/sepe_reg.PNG" alt="Alt text" title="regression"></p>
<p>The question will follow that how to judge if the linear regression really works? I.e., is a linear relationship between X and y justified?</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Initially the budget function aims to find the relation between sale data and “labor goal”, my first thought is to do linear regression without any transformation.&lt;br&gt;
    
    </summary>
    
      <category term="Research" scheme="http://hcuny.github.io/categories/Research/"/>
    
      <category term="Thesis" scheme="http://hcuny.github.io/categories/Research/Thesis/"/>
    
    
      <category term="Statistics" scheme="http://hcuny.github.io/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>Summing Up Sampling</title>
    <link href="http://hcuny.github.io/2017/03/13/sampling/"/>
    <id>http://hcuny.github.io/2017/03/13/sampling/</id>
    <published>2017-03-13T18:13:01.000Z</published>
    <updated>2017-03-16T19:31:11.231Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-Why-Sampling"><a href="#1-Why-Sampling" class="headerlink" title="1. Why Sampling"></a>1. Why Sampling</h3><p>A misconception I used to have is that the era of big data means the end of a need for sampling, actually, in a Big Data project, like the Bosch production line performance prediction, our models are still developed and piloted with samples. More generally speaking, to understand a statistical task, most times we have to design experiments which will inevitably use sampling.<br><a id="more"></a><br>A sample is drawn with the goal of measuring something(with a sample statistic), or modeling something with a statistical/machine learning model. Much of classical statistics is concerned with making inference from small samples to populations: we do that by constructing <strong>Statistic</strong>, i.e. function of sample data to measure different features of population. Naturally we need to analyze sampling distribution, it refers to the distribution of some <strong>Sample Statistic</strong>.</p>
<h3 id="2-Supporting-Theorem"><a href="#2-Supporting-Theorem" class="headerlink" title="2. Supporting Theorem"></a>2. Supporting Theorem</h3><p><a href="https://en.wikipedia.org/wiki/Law_of_large_numbers" target="_blank" rel="external">LLW</a><br><img src="/img/lln.png" alt="Alt text"><br><a href="https://en.wikipedia.org/wiki/Central_limit_theorem" target="_blank" rel="external">CLT</a><br><img src="/img/clt.png" alt="Alt text"></p>
<h3 id="3-Magic-–-The-Bootstrap"><a href="#3-Magic-–-The-Bootstrap" class="headerlink" title="3. Magic – The Bootstrap"></a>3. Magic – The Bootstrap</h3><p>Key Idea: <strong>Treat the sample as if it were the population</strong><br>Think about this: you got 2 samples s1 &amp; s2 with the same size, they have the same point estimate of population mean, but std(s1)&gt;&gt;std(s2), how would you judge the accuracy of the point estimation? Obviously, you will trust s2 more. That is why bootstrap sampling works, it’s actually extracting as much information as possible from the sample.</p>
<p><img src="/img/bootstrap.png" alt="Alt text"></p>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;1-Why-Sampling&quot;&gt;&lt;a href=&quot;#1-Why-Sampling&quot; class=&quot;headerlink&quot; title=&quot;1. Why Sampling&quot;&gt;&lt;/a&gt;1. Why Sampling&lt;/h3&gt;&lt;p&gt;A misconception I used to have is that the era of big data means the end of a need for sampling, actually, in a Big Data project, like the Bosch production line performance prediction, our models are still developed and piloted with samples. More generally speaking, to understand a statistical task, most times we have to design experiments which will inevitably use sampling.&lt;br&gt;
    
    </summary>
    
      <category term="Study Notes" scheme="http://hcuny.github.io/categories/Study-Notes/"/>
    
    
      <category term="Statistics" scheme="http://hcuny.github.io/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>Demand Distribution</title>
    <link href="http://hcuny.github.io/2017/03/12/Demand-Distribution/"/>
    <id>http://hcuny.github.io/2017/03/12/Demand-Distribution/</id>
    <published>2017-03-12T06:09:56.000Z</published>
    <updated>2017-03-16T18:37:35.505Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Number-of-Data-Points-to-Estimate"><a href="#Number-of-Data-Points-to-Estimate" class="headerlink" title="Number of Data Points to Estimate?"></a>Number of Data Points to Estimate?</h4><p>Overall, it’s very well normal-shaped, with a little bit “long tail”.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sns.distplot(df_demand[<span class="string">'actual_sales'</span>].values,bins=<span class="number">100</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/img/overall_density.png" alt="Alt text" title="Overall"><br><a id="more"></a><br>But divided them into stores, take first 5 as example, we’ll find their shape has a lot variation.</p>
<p><img src="/img/single5.png" alt="Alt text" title="5 stores"><br>Now think about our logic: we’re not assuming the actual-sale value is normally distributed, which is a too strong assumption. Instead, we’re assuming the “error of plan”: (actual_sale - manager_prediction) is normally distributed for each store.</p>
<p><img src="/img/diffall.png" alt="Alt text" title="5 stores"><br>Wonderful, it’s just beautiful as we expected. And also take a look at the error of the first 5 stores.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> list(set(df_demand[<span class="string">'storeid'</span>].values))[:<span class="number">5</span>]:</div><div class="line">    value=df_demand[df_demand[<span class="string">'storeid'</span>]==i][<span class="string">'diff'</span>].values</div><div class="line">    sns.distplot(value,bins=<span class="number">20</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/img/diff5.png" alt="Alt text" title="5 stores"><br>Here we comes the question: how many data points is adequate to estimate distribution? The rule of thumb is the more data you have, the better. In most cases, to get reliable distribution fitting results, you should have at least 75-100 data points available. It seems that to cluster stores will be necessary.</p>
<h4 id="A-Detailed-Examination"><a href="#A-Detailed-Examination" class="headerlink" title="A Detailed Examination"></a>A Detailed Examination</h4><p>To test normality of the errors, let’s see the QQ-plot, here is a good link to understand the shape of <a href="https://xiongge.shinyapps.io/QQplots/" target="_blank" rel="external">qq-plot</a>.</p>
<p><img src="/img/diffqq.PNG" alt="Alt text" title="Q-Q Plot"><br>This graph is telling us that our error is still a bit <strong>heavy in the tail</strong>, that is the expected value of the normal distribution in large/small quantiles have a more tight range than the real data(why?). One possible explanation is that, some stores are newly opened s.t. they didn’t have much historical data, which makes their prediction less accurate &amp; lot more variation.</p>
<p>More precise tests like Shapiro-Wilk / Kolmogorov-Smirnov are also needed:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">ks_results = scipy.stats.kstest(df_demand[<span class="string">'diff'</span>], cdf=<span class="string">'norm'</span>)</div><div class="line">matrix_ks = [</div><div class="line">    [<span class="string">''</span>, <span class="string">'DF'</span>, <span class="string">'Test Statistic'</span>, <span class="string">'p-value'</span>],</div><div class="line">    [<span class="string">'Sample Data'</span>, len(df_demand[<span class="string">'diff'</span>]) - <span class="number">1</span>, ks_results[<span class="number">0</span>], ks_results[<span class="number">1</span>]]]</div><div class="line">ks_table = FF.create_table(matrix_ks, index=<span class="keyword">True</span>)</div><div class="line">py.iplot(ks_table, filename=<span class="string">'ks-table'</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/img/normaltall.png" alt="Alt text" title="Kolmogorov-Smirnov T"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">matrix_sw = [[<span class="string">'Store_id'</span>, <span class="string">'DF'</span>, <span class="string">'Test Statistic'</span>, <span class="string">'p-value'</span>]]</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> list(set(df_demand[<span class="string">'storeid'</span>].values))[:<span class="number">10</span>]:</div><div class="line">    shapiro_results = scipy.stats.shapiro(df_demand[df_demand[<span class="string">'storeid'</span>]==i][<span class="string">'diff'</span>])</div><div class="line">    shapiro_results</div><div class="line">    matrix_sw.append(</div><div class="line">        [i, len(df_demand[df_demand[<span class="string">'storeid'</span>]==i][<span class="string">'diff'</span>]) - <span class="number">1</span>, shapiro_results[<span class="number">0</span>], shapiro_results[<span class="number">1</span>]])</div><div class="line">shapiro_table = FF.create_table(matrix_sw, index=<span class="keyword">True</span>)</div><div class="line">py.iplot(shapiro_table, filename=<span class="string">'shapiro-table'</span>)</div></pre></td></tr></table></figure>
<p><img src="/img/normal10.png" alt="Alt text" title="Shapiro-Wilk first 10"><br>First 10 stores’ test results, we may reject them because of the lack of data. <strong>Warning: Clustering Needed</strong>.</p>
<h4 id="Density-Estimation"><a href="#Density-Estimation" class="headerlink" title="Density Estimation"></a>Density Estimation</h4><p><a href="https://en.wikipedia.org/wiki/Kernel_density_estimation" target="_blank" rel="external">KDE</a> will be our tool for this task. Take a look at what the cdf will look like.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">df_demand[<span class="string">'diff'</span>]=df_demand[<span class="string">'actual_sales'</span>]-df_demand[<span class="string">'sales_manageradj'</span>]</div><div class="line">sns.kdeplot(df_demand[<span class="string">'diff'</span>].values, cumulative=<span class="keyword">True</span>)</div></pre></td></tr></table></figure></p>
<p><img src="/img/cdfdiff.png" alt="Alt text" title="cdf plot"><br>Use scipy to calculate the CDF value for a certain point given the kde.<br><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">v=<span class="number">10000</span></div><div class="line">X=np.array(df_demand[<span class="string">'actual_sales'</span>].values)</div><div class="line">gkde=stats.gaussian_kde(X)</div><div class="line">gkde.integrate_box_1d(<span class="number">0</span>,v)</div></pre></td></tr></table></figure></p>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Number-of-Data-Points-to-Estimate&quot;&gt;&lt;a href=&quot;#Number-of-Data-Points-to-Estimate&quot; class=&quot;headerlink&quot; title=&quot;Number of Data Points to Estimate?&quot;&gt;&lt;/a&gt;Number of Data Points to Estimate?&lt;/h4&gt;&lt;p&gt;Overall, it’s very well normal-shaped, with a little bit “long tail”.&lt;br&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;sns.distplot(df_demand[&lt;span class=&quot;string&quot;&gt;&#39;actual_sales&#39;&lt;/span&gt;].values,bins=&lt;span class=&quot;number&quot;&gt;100&lt;/span&gt;)&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/overall_density.png&quot; alt=&quot;Alt text&quot; title=&quot;Overall&quot;&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Research" scheme="http://hcuny.github.io/categories/Research/"/>
    
      <category term="Thesis" scheme="http://hcuny.github.io/categories/Research/Thesis/"/>
    
    
      <category term="Statistics" scheme="http://hcuny.github.io/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>OLS, GLS, WLS, PLS, LARS and ALS</title>
    <link href="http://hcuny.github.io/2017/03/11/Comparison-of-Some-Algorithms/"/>
    <id>http://hcuny.github.io/2017/03/11/Comparison-of-Some-Algorithms/</id>
    <published>2017-03-11T19:54:42.000Z</published>
    <updated>2017-03-14T21:46:13.804Z</updated>
    
    <content type="html"><![CDATA[<p>Some concepts that look similar may lead to confusion, especially when given their abbreviations. This article will try to distinguish <strong>OLS, GLS, WLS, LARS, ALS</strong><br><a id="more"></a></p>
<h3 id="1-OLS-Ordinary-Least-Square"><a href="#1-OLS-Ordinary-Least-Square" class="headerlink" title="1. OLS - Ordinary Least Square"></a>1. OLS - Ordinary Least Square</h3><ul>
<li>No Comment.</li>
</ul>
<h3 id="2-GLS-Generalized-Least-Square"><a href="#2-GLS-Generalized-Least-Square" class="headerlink" title="2. GLS - Generalized Least Square"></a>2. GLS - Generalized Least Square</h3><p>Here we’re not assuming errors are constant and uncorrelated, instead:<br><img src="http://chart.googleapis.com/chart?cht=tx&chl= Var(\epsilon) = \sigma^{2}\Sigma" style="border:none;"><br>Find S as the triangular matrix using the Choleski decomposition.(Square root of error covariance matrix), and reconstruct the regression function, to get constant &amp; uncorrelated error variance.</p>
<h3 id="3-WLS-Weighted-Least-Square"><a href="#3-WLS-Weighted-Least-Square" class="headerlink" title="3. WLS - Weighted Least Square"></a>3. WLS - Weighted Least Square</h3><p>A special case of GLS, errors are uncorrelated but have non-equal variance.</p>
<h3 id="4-PLS-Partial-Least-Square"><a href="#4-PLS-Partial-Least-Square" class="headerlink" title="4. PLS - Partial Least Square"></a>4. PLS - Partial Least Square</h3><p>Same idea as <a href="https://en.wikipedia.org/wiki/Principal_component_regression" target="_blank" rel="external">PCR</a>(Principle Component Regression), the difference is that, PLS also choose what the component is to predict Y, just as ordinary linear regression.<br><img src="/img/PLS.PNG" alt="Alt text" title="Illustration"></p>
<h3 id="5-LARS-Least-Angle-regression"><a href="#5-LARS-Least-Angle-regression" class="headerlink" title="5. LARS - Least Angle regression"></a>5. LARS - Least Angle regression</h3><p>It’s an algorithm for computing linear regression with L1 regularization(Lasso).<br>Big idea: Move coef along the most correlated feature until another feature becomes more correlated.</p>
<ul>
<li>Start with all coef 0</li>
<li>Find feature x_i most correlated with y</li>
<li>Increase corresponding b_i, take along residual r. Stop when some x_j has much correlation with r as x_i</li>
<li>Increase (b_i, b_j) in their joint least square direction, until some other x_m has much correlation with r.</li>
<li>If a non-zero coef hits 0, remove it from the active set of features and recompute the joint direction.</li>
<li>Continue until all features are in the model.<br><img src="/img/Lars.png" alt="Alt text"><br>Generally speaking, it is a forward selection algorithm implemented in Lasso mode. I’ll talk about Ridge, Lasso and ElasticNet regularizations in another article.</li>
</ul>
<h3 id="6-ALS-Alternative-Least-Square"><a href="#6-ALS-Alternative-Least-Square" class="headerlink" title="6. ALS - Alternative Least Square"></a>6. ALS - Alternative Least Square</h3><p>Used in collaborative filtering, powerful technique in building recommendation systems.<br>Here we want UV to be as close to R as possible, since U, V are too low rank, a perfect solution will be impossible. But it’s actually a good thing, which will reduce the iteration rounds of ALS.<br><img src="/img/als-illustration.png" alt="Alt text" title="coefficient path"><br>General idea of ALS: Fix one matrix to optimize another, and do iterations. It’s just like EM algorithm, our objective function here is the loss(difference) between LHS and RHS.</p>
<p>I’ll talk about collaborative filtering along with recommendation system and RSA next time.  </p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Some concepts that look similar may lead to confusion, especially when given their abbreviations. This article will try to distinguish &lt;strong&gt;OLS, GLS, WLS, LARS, ALS&lt;/strong&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Study Notes" scheme="http://hcuny.github.io/categories/Study-Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://hcuny.github.io/tags/Machine-Learning/"/>
    
      <category term="Statistics" scheme="http://hcuny.github.io/tags/Statistics/"/>
    
  </entry>
  
  <entry>
    <title>Master Thesis Proposal</title>
    <link href="http://hcuny.github.io/2017/03/09/Proposal/"/>
    <id>http://hcuny.github.io/2017/03/09/Proposal/</id>
    <published>2017-03-10T04:30:30.000Z</published>
    <updated>2017-03-15T00:41:17.326Z</updated>
    
    <content type="html"><![CDATA[<ul>
<li>This work is under guidance of Professor <a href="http://www.kenan-flagler.unc.edu/faculty/directory/operations-technology-and-innovation-management-otim/saravanan-kesavan" target="_blank" rel="external">Saravanan Kesavan</a> in UNC Kenan-Flagler Business School.</li>
</ul>
<p>Given a retailer’s data across 123 stores through 47 weeks, we want to apply the Newsvendor model to estimate service level for managers in each store. Further research will be, to discover what kind of factors are affecting manager service level.<br><a id="more"></a></p>
<h4 id="1-Newsvendor-Model-General-Summary"><a href="#1-Newsvendor-Model-General-Summary" class="headerlink" title="1. Newsvendor Model General Summary"></a>1. <a href="https://en.wikipedia.org/wiki/Newsvendor_model" target="_blank" rel="external">Newsvendor Model</a> General Summary</h4><p><strong>E[Profit] = E[p*min(q, D)-cq]</strong><br>Here c, p is the fixed cost and price for each unit of demand, q is the number of units stocked(q* will be the best decision we make about inventory amount, it is what we’re interested in). D is a random variable with CDF F representing demand.<br>The solution q<em> satisfy: F(q</em>)= (p-c)/p<br><img src="/img/nvd_2.jpg" alt="Alt text" title="Illustration"></p>
<p>Starting from the very basic case above, in our research, the newsvendor problem assumes only overage and underage costs (denoted C_o and C_u, respectively). It’s not hard to see that C_o ~ c, C_u ~ (p-c). Thus F(Q*)=1/(1+C_o/C_u). The ratio is referred to as “Service Level”.</p>
<ul>
<li>Notes: Derivation of the result. Take the expectation of the loss as our objective function L, we want to minimize it. Let Q be the inventory amount, x is the real demand.(R.V.). Then the loss will be:<ul>
<li>l1(Q, x) = (Q - x)C_o, if x&lt;=Q</li>
<li>l2(Q, x) = (x - Q)C_u, if x&gt;Q<br>Now the objective function is L(Q) =  Int_x_0_Q( l1(Q, x)*f(x) ) + Int_x_Q_inf( l2(Q, x)*f(x) ), let L’(Q) = 0, we will get the result shown above.</li>
</ul>
</li>
</ul>
<h4 id="2-How-We’ll-Apply-the-Model"><a href="#2-How-We’ll-Apply-the-Model" class="headerlink" title="2. How We’ll Apply the Model"></a>2. How We’ll Apply the Model</h4><p>Different from direct application, a twist in this research is that we’re interested in the “best labor spend”(i.e the money spent on labor) for each store, rather than the inventory amount. In the standpoint of a store manager, similar to inventory, overly or underly arrange the labor than the real need will bring loss. Therefore, the service level of a manager measures his/her labor arrangement strategy.</p>
<p>For each week, the labor_goal column in the data refers to the labor cost planned to be made based on the actual sale data, and actual_labor refers to the actual optimized labor cost L*. Thus, if we can get the labor cost distribution F(l), given the known L*, we can calculate the service level directly.</p>
<p>Now problem comes to how can we find the labor cost distribution? The idea is, given that we’re able to find the demand distribution, and if we know some relationship between the demand distribution and labor distribution, then we can find the labor distribution as well. The connection here is made by <strong>Budget Function</strong>, it takes the actual labor as input and labor goal as output. This function is a measure of “how much you should have spent on labor” given the actual sale.</p>
<ul>
<li>Notes: Demand is completely measured by sale, thus we’re not distinguishing the two concepts. Demand is a random variable, and we’re assuming <strong>demand ~ sale forecast + random noise</strong>, thus we’re going to find the pattern of random noise in the first place. The sale forecast is measured by the sales_manageradj column.</li>
</ul>
<h4 id="3-Steps-To-Go"><a href="#3-Steps-To-Go" class="headerlink" title="3. Steps To Go"></a>3. Steps To Go</h4><ul>
<li>First thing to do is finding the demand distribution F(x) for each store.</li>
<li>Given the labor_goal data, and sale data, find the labor budget function B.</li>
<li>Using demand distribution + budget function, get the labor distribution F(l)</li>
<li>Using the given L* to find service level for each store(manager).</li>
<li>Find factors that affect the service level.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;ul&gt;
&lt;li&gt;This work is under guidance of Professor &lt;a href=&quot;http://www.kenan-flagler.unc.edu/faculty/directory/operations-technology-and-innovation-management-otim/saravanan-kesavan&quot;&gt;Saravanan Kesavan&lt;/a&gt; in UNC Kenan-Flagler Business School.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given a retailer’s data across 123 stores through 47 weeks, we want to apply the Newsvendor model to estimate service level for managers in each store. Further research will be, to discover what kind of factors are affecting manager service level.&lt;br&gt;
    
    </summary>
    
      <category term="Research" scheme="http://hcuny.github.io/categories/Research/"/>
    
      <category term="Thesis" scheme="http://hcuny.github.io/categories/Research/Thesis/"/>
    
    
      <category term="Statistics" scheme="http://hcuny.github.io/tags/Statistics/"/>
    
      <category term="Operations" scheme="http://hcuny.github.io/tags/Operations/"/>
    
      <category term="Management" scheme="http://hcuny.github.io/tags/Management/"/>
    
  </entry>
  
  <entry>
    <title>Basic Elements in TF</title>
    <link href="http://hcuny.github.io/2017/03/08/Tensor/"/>
    <id>http://hcuny.github.io/2017/03/08/Tensor/</id>
    <published>2017-03-09T01:32:35.000Z</published>
    <updated>2017-03-14T21:47:25.350Z</updated>
    
    <content type="html"><![CDATA[<p>Struggling with the second project of the Udacity course, image classification, it’s a key point to understand the process as a whole. Starting with:</p>
<h4 id="1-What-is-a-tensor"><a href="#1-What-is-a-tensor" class="headerlink" title="1. What is a tensor?"></a>1. What is a tensor?</h4><p>First of all, consider everything in a graph mode. <br><br><a id="more"></a><br>TensorFlow is a programming system in which you <strong>represent computations</strong> as graphs. <strong>Nodes</strong> in the graph are called ops ( operations). An op takes zero or more Tensors, performs some computation, and produces zero or more Tensors. A Tensor is a typed <strong>multi-dimensional array</strong>. For example, you can represent a mini-batch of images as a 4-D array of floating point numbers with dimensions [batch, height, width, channels]</p>
<p><img src="/img/tensor.jpg" alt="Alt text" title="different shapes of tensors"><br>Then we can claim: TensorFlow ~ Tensor + Flow <br><br>I.e. tensors (units of array-like data) flowing through nodes (different kinds of operations, inner product, sigmoid, softmax, relu, etc), all the nodes forms a neural network graph. <a href="https://github.com/hcuny/Deep-Learning-Projects-Udacity/blob/master/Projects/dlnd-your-first-neural-network.ipynb" target="_blank" rel="external">Demo</a> of a small NN.</p>
<p><img src="/img/nn.png" alt="Alt text"></p>
<h4 id="2-What-is-a-session"><a href="#2-What-is-a-session" class="headerlink" title="2. What is a session?"></a>2. What is a session?</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sess = tf.Session()</div><div class="line">$ sess.run(x, feed_dict=&#123;~&#125;)</div></pre></td></tr></table></figure>
<p>It’s actually bringing the graph framework into implementation. <br><br>On the other hand, like the lazy computation of Spark RDDs, <strong>tf.placeholder()</strong> simply allocates a block of memory for future use in sess.run(), thus the computation graph/pipeline can be built ahead of any real data flow. Example of some commands that are part of the graph:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ input = tf.placeholder(tf.float32, (None, h, w, d))</div><div class="line">$ filter_weights = tf.Variable(tf.truncated_normal((H, W, in_d, out_d)))</div><div class="line">$ bias = tf.Variable(tf.zeros(conv_num_outputs))</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding=<span class="string">'SAME'</span>)</div><div class="line">$ conv_layer = tf.nn.bias_add(conv_layer, bias)</div><div class="line">$ conv_layer = tf.nn.relu(conv_layer)</div><div class="line">$ conv_layer = tf.nn.max_pool( conv_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=<span class="string">'SAME'</span>)</div><div class="line">$ session.run(optimizer, feed_dict=&#123;x: feature_batch, y: label_batch, keep_prob: keep_probability&#125;)</div></pre></td></tr></table></figure>
<h4 id="3-Why-CNN"><a href="#3-Why-CNN" class="headerlink" title="3. Why CNN?"></a>3. Why CNN?</h4><p><img src="/img/mdnet.png" alt="Alt text"></p>
<ul>
<li>Regular neural nets: nodes in a single layer are independent, will generate huge scale of weights.</li>
<li>Full connectivity is a waste of “adjacent” info, number of parameters lead to overfitting.</li>
<li>The depth of filter is just like different nodes in a single layer.(To capture different levels of info)</li>
<li>Number of filters ~ depth of filters</li>
<li>For the same filter (same depth), share all parameters. Example:<ul>
<li>Input shape 32x32x3 (H, W, D_Channel)</li>
<li>20 filters of shape 8x8x3, stride 2, padding 1.</li>
<li>Output shape -&gt; 14x14x20</li>
<li>No P sharing: (8x8x3+1)x(14x14x20)</li>
<li>With P sharing: (8x8x3+1)x20</li>
</ul>
</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Struggling with the second project of the Udacity course, image classification, it’s a key point to understand the process as a whole. Starting with:&lt;/p&gt;
&lt;h4 id=&quot;1-What-is-a-tensor&quot;&gt;&lt;a href=&quot;#1-What-is-a-tensor&quot; class=&quot;headerlink&quot; title=&quot;1. What is a tensor?&quot;&gt;&lt;/a&gt;1. What is a tensor?&lt;/h4&gt;&lt;p&gt;First of all, consider everything in a graph mode. &lt;br /&gt;&lt;br&gt;
    
    </summary>
    
      <category term="Study Notes" scheme="http://hcuny.github.io/categories/Study-Notes/"/>
    
    
      <category term="Deep Learning" scheme="http://hcuny.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>SQL queries review</title>
    <link href="http://hcuny.github.io/2017/03/08/SQL_query/"/>
    <id>http://hcuny.github.io/2017/03/08/SQL_query/</id>
    <published>2017-03-08T17:32:35.000Z</published>
    <updated>2017-03-14T21:47:14.757Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Concept-Review"><a href="#Concept-Review" class="headerlink" title="Concept Review"></a>Concept Review</h3><ul>
<li>What does (relational) DB bring?<ul>
<li>It’s actually a collection of info organized to afford efficient retrieval.</li>
<li>DBMS: software packages designed to store, access, manage DBs.</li>
</ul>
</li>
<li>In relational DB:<ul>
<li><strong>Every thing is table.</strong></li>
<li>Under the basic structure, relational algebra is working, closure &amp; optimize.</li>
<li>Always keep in mind of algebra optimization when querying.<a id="more"></a>
| DBMS | Database | Schema | Table | Attribute |<br>| — | — | — | — | — |<br>| Property Firm | House | Floor Plan | Room | Decoration |</li>
</ul>
</li>
</ul>
<h3 id="Join"><a href="#Join" class="headerlink" title="Join"></a>Join</h3><ul>
<li><p>Be used to apply Non-Equi-Joins</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">select</span> e.emp_id, e.fname</div><div class="line"><span class="keyword">from</span> employee e <span class="keyword">Inner</span> <span class="keyword">Join</span> product p</div><div class="line"><span class="keyword">On</span> e.s_date&gt;= p.date_offer</div><div class="line"><span class="keyword">And</span> e.s_date&lt; p.date_retire</div></pre></td></tr></table></figure>
</li>
<li><p>Self-Joins and use subqueries as tables</p>
</li>
</ul>
<h3 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a>Aggregation</h3><p>Be careful about implicit/explicit groups.<br>Example 1: group ~ product_cd = ‘CHK’<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> <span class="keyword">MAX</span>(avail_balance) max_balance,</div><div class="line">-&gt; <span class="keyword">MIN</span>(avail_balance) min_balance,</div><div class="line">-&gt; <span class="keyword">AVG</span>(avail_balance) avg_balance,</div><div class="line">-&gt; <span class="keyword">SUM</span>(avail_balance) tot_balance,</div><div class="line">-&gt; <span class="keyword">COUNT</span>(*) num_accounts</div><div class="line">-&gt; <span class="keyword">FROM</span> <span class="keyword">account</span></div><div class="line">-&gt; <span class="keyword">WHERE</span> product_cd = <span class="string">'CHK'</span>;</div></pre></td></tr></table></figure></p>
<p>Example 2: group ~  product_cd<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> product_cd,</div><div class="line">-&gt; <span class="keyword">MAX</span>(avail_balance) max_balance,</div><div class="line">-&gt; <span class="keyword">MIN</span>(avail_balance) min_balance,</div><div class="line">-&gt; <span class="keyword">AVG</span>(avail_balance) avg_balance,</div><div class="line">-&gt; <span class="keyword">SUM</span>(avail_balance) tot_balance,</div><div class="line">-&gt; <span class="keyword">COUNT</span>(*) num_accts</div><div class="line">-&gt; <span class="keyword">FROM</span> <span class="keyword">account</span></div><div class="line">-&gt; <span class="keyword">GROUP</span> <span class="keyword">BY</span> product_cd;</div></pre></td></tr></table></figure></p>
<p>Example 3: Multicolumn Grouping, understanding: what being grouped on will perform as “P Key” on the resulting table.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> product_cd, open_branch_id,</div><div class="line">-&gt; <span class="keyword">SUM</span>(avail_balance) tot_balance</div><div class="line">-&gt; <span class="keyword">FROM</span> <span class="keyword">account</span></div><div class="line">-&gt; <span class="keyword">GROUP</span> <span class="keyword">BY</span> product_cd, open_branch_id;</div></pre></td></tr></table></figure></p>
<p>Example 4: Expressions<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> <span class="keyword">MAX</span>(pending_balance - avail_balance) max_uncleared</div><div class="line">-&gt; <span class="keyword">FROM</span> <span class="keyword">account</span>;</div></pre></td></tr></table></figure></p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> <span class="keyword">EXTRACT</span>(<span class="keyword">YEAR</span> <span class="keyword">FROM</span> start_date) <span class="keyword">year</span>,</div><div class="line">-&gt; <span class="keyword">COUNT</span>(*) how_many</div><div class="line">-&gt; <span class="keyword">FROM</span> employee</div><div class="line">-&gt; <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">EXTRACT</span>(<span class="keyword">YEAR</span> <span class="keyword">FROM</span> start_date);</div></pre></td></tr></table></figure>
<h3 id="String-manipulation-RE"><a href="#String-manipulation-RE" class="headerlink" title="String manipulation (RE)"></a>String manipulation (RE)</h3><p><strong>Keyword: Like</strong><br>| Search Expression | Interpretation |<br>| — | — |<br>| F%                | Strings beginning with F                              |<br>| %t                | Strings ending with t                                 |<br>| %bas%             | Strings containing the substring ‘bas’                |<br>| __t_             | Four-character strings with a t in the third position |</p>
<p><strong>Keyword: REGEXP</strong><br>E.g. Find all employees whose last name starts with F or G.<br><figure class="highlight sql"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">SELECT</span> emp_id, fname, lname</div><div class="line"><span class="keyword">FROM</span> employee</div><div class="line"><span class="keyword">WHERE</span> lname REGEXP <span class="string">'^[FG]'</span>;</div></pre></td></tr></table></figure></p>
<h3 id="Mistakes-collection-when-Writing-queries"><a href="#Mistakes-collection-when-Writing-queries" class="headerlink" title="Mistakes collection when Writing queries"></a>Mistakes collection when Writing queries</h3><ul>
<li>If you want to generate a column/set, must select the data from table in the first place.<br><a href="https://leetcode.com/problems/customers-who-never-order/?tab=Description" target="_blank" rel="external">Demo</a>.</li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;Concept-Review&quot;&gt;&lt;a href=&quot;#Concept-Review&quot; class=&quot;headerlink&quot; title=&quot;Concept Review&quot;&gt;&lt;/a&gt;Concept Review&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;What does (relational) DB bring?&lt;ul&gt;
&lt;li&gt;It’s actually a collection of info organized to afford efficient retrieval.&lt;/li&gt;
&lt;li&gt;DBMS: software packages designed to store, access, manage DBs.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;In relational DB:&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Every thing is table.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Under the basic structure, relational algebra is working, closure &amp;amp; optimize.&lt;/li&gt;
&lt;li&gt;Always keep in mind of algebra optimization when querying.
    
    </summary>
    
      <category term="Review" scheme="http://hcuny.github.io/categories/Review/"/>
    
    
      <category term="Database" scheme="http://hcuny.github.io/tags/Database/"/>
    
      <category term="Interview" scheme="http://hcuny.github.io/tags/Interview/"/>
    
  </entry>
  
  <entry>
    <title>Some Thinking on Tree Based Model</title>
    <link href="http://hcuny.github.io/2017/03/08/Tree_Model_Thinking/"/>
    <id>http://hcuny.github.io/2017/03/08/Tree_Model_Thinking/</id>
    <published>2017-03-08T07:32:35.000Z</published>
    <updated>2017-03-14T21:47:32.050Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Question1-How-does-ensemble-of-different-tree-based-models-enhance-the-performance"><a href="#Question1-How-does-ensemble-of-different-tree-based-models-enhance-the-performance" class="headerlink" title="Question1: How does ensemble of different tree-based models enhance the performance?"></a>Question1: How does ensemble of different tree-based models enhance the performance?</h4><h4 id="Question2-For-important-but-also-highly-correlated-features-how-do-they-affect-performance"><a href="#Question2-For-important-but-also-highly-correlated-features-how-do-they-affect-performance" class="headerlink" title="Question2: For important but also highly correlated features, how do they affect performance?"></a>Question2: For important but also highly correlated features, how do they affect performance?</h4><a id="more"></a>
]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Question1-How-does-ensemble-of-different-tree-based-models-enhance-the-performance&quot;&gt;&lt;a href=&quot;#Question1-How-does-ensemble-of-different-tree-based-models-enhance-the-performance&quot; class=&quot;headerlink&quot; title=&quot;Question1: How does ensemble of different tree-based models enhance the performance?&quot;&gt;&lt;/a&gt;Question1: How does ensemble of different tree-based models enhance the performance?&lt;/h4&gt;&lt;h4 id=&quot;Question2-For-important-but-also-highly-correlated-features-how-do-they-affect-performance&quot;&gt;&lt;a href=&quot;#Question2-For-important-but-also-highly-correlated-features-how-do-they-affect-performance&quot; class=&quot;headerlink&quot; title=&quot;Question2: For important but also highly correlated features, how do they affect performance?&quot;&gt;&lt;/a&gt;Question2: For important but also highly correlated features, how do they affect performance?&lt;/h4&gt;
    
    </summary>
    
      <category term="Study Notes" scheme="http://hcuny.github.io/categories/Study-Notes/"/>
    
    
      <category term="Machine Learning" scheme="http://hcuny.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Introduction</title>
    <link href="http://hcuny.github.io/2017/03/05/First-post/"/>
    <id>http://hcuny.github.io/2017/03/05/First-post/</id>
    <published>2017-03-06T01:32:35.000Z</published>
    <updated>2017-03-07T20:47:00.841Z</updated>
    
    <content type="html"><![CDATA[<p>This is a personal tech blog. On the way of becoming a data scientist.</p>
<p>To properly use <a href="https://probberechts.github.io/cactus-dark/2016/11/01/An-overview-of-all-Markdown-elements/" target="_blank" rel="external">Markdown</a>.</p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;This is a personal tech blog. On the way of becoming a data scientist.&lt;/p&gt;
&lt;p&gt;To properly use &lt;a href=&quot;https://probberechts.github.io/cac
    
    </summary>
    
    
      <category term="Tea Hour" scheme="http://hcuny.github.io/tags/Tea-Hour/"/>
    
  </entry>
  
  <entry>
    <title>Get Started With HEXO</title>
    <link href="http://hcuny.github.io/2017/03/05/hello-world/"/>
    <id>http://hcuny.github.io/2017/03/05/hello-world/</id>
    <published>2017-03-06T00:44:47.163Z</published>
    <updated>2017-03-14T21:46:52.218Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.<br><a id="more"></a></p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>
]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot;&gt;GitHub&lt;/a&gt;.&lt;br&gt;
    
    </summary>
    
    
      <category term="Tea Hour" scheme="http://hcuny.github.io/tags/Tea-Hour/"/>
    
  </entry>
  
</feed>
