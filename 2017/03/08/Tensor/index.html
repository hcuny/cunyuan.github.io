<!DOCTYPE html><html><head><meta charset="UTF-8"><title>Basic Elements in TF</title><link rel="stylesheet" href="http://obfnm2kw5.bkt.clouddn.com/normalize.css"><link rel="stylesheet" href="/css/hexo-theme-adoubi.css"><link rel="icon" href="/images/favicon.ico"></head><body><div class="header"><div class="header__title"><a href="/"><figure class="header__title__image"></figure></a></div><div class="header__archive"><a href="/archives"><figure class="header__archive__image"></figure><span class="header__archive__title">archive</span></a></div><div class="header__about"><a href="/about"><figure class="header__about__image"></figure><span class="header__about__title">about</span></a></div><div class="header__github"><a href="https://github.com/shinux" target="_blank"><figure class="header__github__image"></figure><span class="header__github__title">github</span></a></div><div class="header__feed"><a href="/atom.xml"><figure class="header__feed__image"></figure><span class="header__feed__title">feed</span></a></div><div class="header__notes"></div></div><div class="content"><div class="post-item"></div><h2 class="post-title-wrapper"><p class="post-title">Basic Elements in TF</p></h2><div class="post-date"><time datetime="2017-03-09T01:32:35.000Z">2017-03-08</time></div><div class="post-content"><p>Struggling with the second project of the Udacity course, image classification, it’s a key point to understand the process as a whole. Starting with:</p>
<h4 id="1-What-is-a-tensor"><a href="#1-What-is-a-tensor" class="headerlink" title="1. What is a tensor?"></a>1. What is a tensor?</h4><p>First of all, consider everything in a graph mode. <br></p>
<p>TensorFlow is a programming system in which you <strong>represent computations</strong> as graphs. <strong>Nodes</strong> in the graph are called ops ( operations). An op takes zero or more Tensors, performs some computation, and produces zero or more Tensors. A Tensor is a typed <strong>multi-dimensional array</strong>. For example, you can represent a mini-batch of images as a 4-D array of floating point numbers with dimensions [batch, height, width, channels]</p>
<p><img src="/img/tensor.jpg" alt="Alt text" title="different shapes of tensors"><br>Then we can claim: TensorFlow ~ Tensor + Flow <br><br>I.e. tensors (units of array-like data) flowing through nodes (different kinds of operations, inner product, sigmoid, softmax, relu, etc), all the nodes forms a neural network graph. <a href="https://github.com/hcuny/Deep-Learning-Projects-Udacity/blob/master/Projects/dlnd-your-first-neural-network.ipynb" target="_blank" rel="external">Demo</a> of a small NN.</p>
<p><img src="/img/nn.png" alt="Alt text"></p>
<h4 id="2-What-is-a-session"><a href="#2-What-is-a-session" class="headerlink" title="2. What is a session?"></a>2. What is a session?</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">$ sess = tf.Session()</div><div class="line">$ sess.run(x, feed_dict=&#123;~&#125;)</div></pre></td></tr></table></figure>
<p>It’s actually bringing the graph framework into implementation. <br><br>On the other hand, like the lazy computation of Spark RDDs, <strong>tf.placeholder()</strong> simply allocates a block of memory for future use in sess.run(), thus the computation graph/pipeline can be built ahead of any real data flow. Example of some commands that are part of the graph:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$ input = tf.placeholder(tf.float32, (None, h, w, d))</div><div class="line">$ filter_weights = tf.Variable(tf.truncated_normal((H, W, in_d, out_d)))</div><div class="line">$ bias = tf.Variable(tf.zeros(conv_num_outputs))</div></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$ conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding=<span class="string">'SAME'</span>)</div><div class="line">$ conv_layer = tf.nn.bias_add(conv_layer, bias)</div><div class="line">$ conv_layer = tf.nn.relu(conv_layer)</div><div class="line">$ conv_layer = tf.nn.max_pool( conv_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=<span class="string">'SAME'</span>)</div><div class="line">$ session.run(optimizer, feed_dict=&#123;x: feature_batch, y: label_batch, keep_prob: keep_probability&#125;)</div></pre></td></tr></table></figure>
<h4 id="3-Why-CNN"><a href="#3-Why-CNN" class="headerlink" title="3. Why CNN?"></a>3. Why CNN?</h4><p><img src="/img/mdnet.png" alt="Alt text"></p>
<ul>
<li>Regular neural nets: nodes in a single layer are independent, will generate huge scale of weights.</li>
<li>Full connectivity is a waste of “adjacent” info, number of parameters lead to overfitting.</li>
<li>The depth of filter is just like different nodes in a single layer.(To capture different levels of info)</li>
<li>Number of filters ~ depth of filters</li>
<li>For the same filter (same depth), share all parameters. Example:<ul>
<li>Input shape 32x32x3 (H, W, D_Channel)</li>
<li>20 filters of shape 8x8x3, stride 2, padding 1.</li>
<li>Output shape -&gt; 14x14x20</li>
<li>No P sharing: (8x8x3+1)x(14x14x20)</li>
<li>With P sharing: (8x8x3+1)x20</li>
</ul>
</li>
</ul>
</div></div><div class="footer"><div class="footer-copyright">Theme By <a href="https://github.com/shinux/hexo-theme-adoubi">Adoubi</a> , Powered By Hexo.</div></div></body></html>