<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Summing Up Clustering]]></title>
      <url>%2F2017%2F03%2F16%2FSumming-Up-Clustering%2F</url>
      <content type="text"><![CDATA[1. OverviewClustering algorithm types1234567891011121314All types│———Prototype Based: point assignment│ | | |———Centroids: distance metric applied to center of a subgroup| || |———Medoids: representative point of the graph |└─── Hierarchical Based│ ││ │———Agglomeration│ ││ └───Division│ └───Density Based 2. Agglomerative algorithmsAgglomerative algorithms start from singletons, and they differ because of the strategy used to merge 2 clusters, i.e. the metric used to calculate the distance between 2 clusters. Let’s see different strategies of merging clusters. Single Link(MIN): find all min distance between any 2 points from two clusters, and compare. Min(min(2clusters))Time complexity O(n^2), we do not need to recompute the pairs each time. Complete Link(MAX/CLIQUE): find all max distance between any 2 points from two clusters. Min(max(2clusters))Time complexity O(n^2 log n), consider the first merge, compute each pair &amp; sort. Group Average: keep updating the centroid of each cluster. Min(d(2centroid)) Ward’s Method: track the increase in squared error when merged. Min(Increase_SE(2clusters)) Real example from the retailer data: 3. Divisive algorithmsInterestingly connected with MST, just consider the whole graph made up of weights(distance between 2 clusters), first find the MST, and break link corresponding to the largest distance. 4. K-means: A better startWhen Initializing k centroids, we’ll not do it randomly, instead: Pick one point randomly into the active set s while len(s) &lt;k, add a point whose minimum distance from s is maximized. Then we’ll follow the normal process, assign each point to the nearest centroid &amp; update the centroids. Note that the global minimization function is min(SSE) = sum(sum(SSE each cluster)) 5. Find the best k?Elbow Method – graphically.Note: this is typically used in K-means for the case that K is unknown, but in the implementation of the algorithm, we shall assign a K value in advance. (Q: We can also use it in Hierarchical?)For Hierarchical clustering, the process can stop when there’re a fixed number of clusters left, or we can stop until there is no further compact merge, which means we need to define a threshold for the merged diameter/radius. 6. Some thinking Hierarchical clustering is relatively fast because of its greedy nature, the process is very much like constructing a Huffman tree. However, we don’t have a global objective function, which means in each merge decision, it’s not made from a globally optimized point of view, the resulting clusters are not even stable. I think this is the key difference distinguishing it from k-means. Computation complexity for hierarchical clustering: In the basic setting, obviously it’s O(n^2)+O((n-1)^2)+O((n-2)^2)… = O(n^3)Now introduce a improved one: initial step: O(n^2), Form the pairs into a priority queue (constant time to find pairs to merge), which takes O(n^2). Here’s the trick: when decide to merge C &amp; D, remove from the priority queue all entries involving C or D (Deletion O(log n), at most 2n deletions). Recompute the pair distance and insert (O(log n) for insertion, and at most n insertions). Thus, overall it’s O(n^2)+O(n^2)+O(n*nlog n)=O(n^2log n). Computation complexity for K-means: Global optimization function NP Hard. The running time of Lloyd’s algorithm is naively O(nkdi), where n is the number of d-dimensional vectors, k the number of clusters and i the number of iterations needed until convergence. Ward’s method can be used as a robust method of Initializing a K-means clustering.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Store Clustering]]></title>
      <url>%2F2017%2F03%2F16%2FStore-Clustering%2F</url>
      <content type="text"><![CDATA[Clustering by geographic dataClustering directly by Sale performance]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Budget Function]]></title>
      <url>%2F2017%2F03%2F14%2Fbudget-function%2F</url>
      <content type="text"><![CDATA[Initially the budget function aims to find the relation between sale data and “labor goal”, my first thought is to do linear regression without any transformation.First take a look at the scatter plot of all data points. For fun, plot the overall regression plot for each store, shown in the same graph. Plot regression forms on each store, note that logically speaking budget function is designed for each store.1sns.lmplot(x="actual_sales", y="labor_goal", col="storeid", data=df_demand, size=3.5,col_wrap=3,aspect=1) The question will follow that how to judge if the linear regression really works? I.e., is a linear relationship between X and y justified?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Summing Up Sampling]]></title>
      <url>%2F2017%2F03%2F13%2Fsampling%2F</url>
      <content type="text"><![CDATA[1. Why SamplingA misconception I used to have is that the era of big data means the end of a need for sampling, actually, in a Big Data project, like the Bosch production line performance prediction, our models are still developed and piloted with samples. More generally speaking, to understand a statistical task, most times we have to design experiments which will inevitably use sampling.A sample is drawn with the goal of measuring something(with a sample statistic), or modeling something with a statistical/machine learning model. Much of classical statistics is concerned with making inference from small samples to populations: we do that by constructing Statistic, i.e. function of sample data to measure different features of population. Naturally we need to analyze sampling distribution, it refers to the distribution of some Sample Statistic. 2. Supporting TheoremLLWCLT 3. Magic – The BootstrapKey Idea: Treat the sample as if it were the populationThink about this: you got 2 samples s1 &amp; s2 with the same size, they have the same point estimate of population mean, but std(s1)&gt;&gt;std(s2), how would you judge the accuracy of the point estimation? Obviously, you will trust s2 more. That is why bootstrap sampling works, it’s actually extracting as much information as possible from the sample.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Demand Distribution]]></title>
      <url>%2F2017%2F03%2F12%2FDemand-Distribution%2F</url>
      <content type="text"><![CDATA[Number of Data Points to Estimate?Overall, it’s very well normal-shaped, with a little bit “long tail”.1sns.distplot(df_demand['actual_sales'].values,bins=100) But divided them into stores, take first 5 as example, we’ll find their shape has a lot variation. Now think about our logic: we’re not assuming the actual-sale value is normally distributed, which is a too strong assumption. Instead, we’re assuming the “error of plan”: (actual_sale - manager_prediction) is normally distributed for each store. Wonderful, it’s just beautiful as we expected. And also take a look at the error of the first 5 stores.123for i in list(set(df_demand['storeid'].values))[:5]: value=df_demand[df_demand['storeid']==i]['diff'].values sns.distplot(value,bins=20) Here we comes the question: how many data points is adequate to estimate distribution? The rule of thumb is the more data you have, the better. In most cases, to get reliable distribution fitting results, you should have at least 75-100 data points available. It seems that to cluster stores will be necessary. A Detailed ExaminationTo test normality of the errors, let’s see the QQ-plot, here is a good link to understand the shape of qq-plot. This graph is telling us that our error is still a bit heavy in the tail, that is the expected value of the normal distribution in large/small quantiles have a more tight range than the real data(why?). One possible explanation is that, some stores are newly opened s.t. they didn’t have much historical data, which makes their prediction less accurate &amp; lot more variation. More precise tests like Shapiro-Wilk / Kolmogorov-Smirnov are also needed:123456ks_results = scipy.stats.kstest(df_demand['diff'], cdf='norm')matrix_ks = [ ['', 'DF', 'Test Statistic', 'p-value'], ['Sample Data', len(df_demand['diff']) - 1, ks_results[0], ks_results[1]]]ks_table = FF.create_table(matrix_ks, index=True)py.iplot(ks_table, filename='ks-table') 12345678matrix_sw = [['Store_id', 'DF', 'Test Statistic', 'p-value']]for i in list(set(df_demand['storeid'].values))[:10]: shapiro_results = scipy.stats.shapiro(df_demand[df_demand['storeid']==i]['diff']) shapiro_results matrix_sw.append( [i, len(df_demand[df_demand['storeid']==i]['diff']) - 1, shapiro_results[0], shapiro_results[1]])shapiro_table = FF.create_table(matrix_sw, index=True)py.iplot(shapiro_table, filename='shapiro-table') First 10 stores’ test results, we may reject them because of the lack of data. Warning: Clustering Needed. Density EstimationKDE will be our tool for this task. Take a look at what the cdf will look like.12df_demand['diff']=df_demand['actual_sales']-df_demand['sales_manageradj']sns.kdeplot(df_demand['diff'].values, cumulative=True) Use scipy to calculate the CDF value for a certain point given the kde.1234v=10000X=np.array(df_demand['actual_sales'].values)gkde=stats.gaussian_kde(X)gkde.integrate_box_1d(0,v)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[OLS, GLS, WLS, PLS, LARS and ALS]]></title>
      <url>%2F2017%2F03%2F11%2FComparison-of-Some-Algorithms%2F</url>
      <content type="text"><![CDATA[Some concepts that look similar may lead to confusion, especially when given their abbreviations. This article will try to distinguish OLS, GLS, WLS, LARS, ALS 1. OLS - Ordinary Least Square No Comment. 2. GLS - Generalized Least SquareHere we’re not assuming errors are constant and uncorrelated, instead:Find S as the triangular matrix using the Choleski decomposition.(Square root of error covariance matrix), and reconstruct the regression function, to get constant &amp; uncorrelated error variance. 3. WLS - Weighted Least SquareA special case of GLS, errors are uncorrelated but have non-equal variance. 4. PLS - Partial Least SquareSame idea as PCR(Principle Component Regression), the difference is that, PLS also choose what the component is to predict Y, just as ordinary linear regression. 5. LARS - Least Angle regressionIt’s an algorithm for computing linear regression with L1 regularization(Lasso).Big idea: Move coef along the most correlated feature until another feature becomes more correlated. Start with all coef 0 Find feature x_i most correlated with y Increase corresponding b_i, take along residual r. Stop when some x_j has much correlation with r as x_i Increase (b_i, b_j) in their joint least square direction, until some other x_m has much correlation with r. If a non-zero coef hits 0, remove it from the active set of features and recompute the joint direction. Continue until all features are in the model.Generally speaking, it is a forward selection algorithm implemented in Lasso mode. I’ll talk about Ridge, Lasso and ElasticNet regularizations in another article. 6. ALS - Alternative Least SquareUsed in collaborative filtering, powerful technique in building recommendation systems.Here we want UV to be as close to R as possible, since U, V are too low rank, a perfect solution will be impossible. But it’s actually a good thing, which will reduce the iteration rounds of ALS.General idea of ALS: Fix one matrix to optimize another, and do iterations. It’s just like EM algorithm, our objective function here is the loss(difference) between LHS and RHS. I’ll talk about collaborative filtering along with recommendation system and RSA next time.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Master Thesis Proposal]]></title>
      <url>%2F2017%2F03%2F09%2FProposal%2F</url>
      <content type="text"><![CDATA[This work is under guidance of Professor Saravanan Kesavan in UNC Kenan-Flagler Business School. Given a retailer’s data across 123 stores through 47 weeks, we want to apply the Newsvendor model to estimate service level for managers in each store. Further research will be, to discover what kind of factors are affecting manager service level. 1. Newsvendor Model General SummaryE[Profit] = E[p*min(q, D)-cq]Here c, p is the fixed cost and price for each unit of demand, q is the number of units stocked(q* will be the best decision we make about inventory amount, it is what we’re interested in). D is a random variable with CDF F representing demand.The solution q satisfy: F(q)= (p-c)/p Starting from the very basic case above, in our research, the newsvendor problem assumes only overage and underage costs (denoted C_o and C_u, respectively). It’s not hard to see that C_o ~ c, C_u ~ (p-c). Thus F(Q*)=1/(1+C_o/C_u). The ratio is referred to as “Service Level”. Notes: Derivation of the result. Take the expectation of the loss as our objective function L, we want to minimize it. Let Q be the inventory amount, x is the real demand.(R.V.). Then the loss will be: l1(Q, x) = (Q - x)C_o, if x&lt;=Q l2(Q, x) = (x - Q)C_u, if x&gt;QNow the objective function is L(Q) = Int_x_0_Q( l1(Q, x)*f(x) ) + Int_x_Q_inf( l2(Q, x)*f(x) ), let L’(Q) = 0, we will get the result shown above. 2. How We’ll Apply the ModelDifferent from direct application, a twist in this research is that we’re interested in the “best labor spend”(i.e the money spent on labor) for each store, rather than the inventory amount. In the standpoint of a store manager, similar to inventory, overly or underly arrange the labor than the real need will bring loss. Therefore, the service level of a manager measures his/her labor arrangement strategy. For each week, the labor_goal column in the data refers to the labor cost planned to be made based on the actual sale data, and actual_labor refers to the actual optimized labor cost L*. Thus, if we can get the labor cost distribution F(l), given the known L*, we can calculate the service level directly. Now problem comes to how can we find the labor cost distribution? The idea is, given that we’re able to find the demand distribution, and if we know some relationship between the demand distribution and labor distribution, then we can find the labor distribution as well. The connection here is made by Budget Function, it takes the actual labor as input and labor goal as output. This function is a measure of “how much you should have spent on labor” given the actual sale. Notes: Demand is completely measured by sale, thus we’re not distinguishing the two concepts. Demand is a random variable, and we’re assuming demand ~ sale forecast + random noise, thus we’re going to find the pattern of random noise in the first place. The sale forecast is measured by the sales_manageradj column. 3. Steps To Go First thing to do is finding the demand distribution F(x) for each store. Given the labor_goal data, and sale data, find the labor budget function B. Using demand distribution + budget function, get the labor distribution F(l) Using the given L* to find service level for each store(manager). Find factors that affect the service level.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Basic Elements in TF]]></title>
      <url>%2F2017%2F03%2F08%2FTensor%2F</url>
      <content type="text"><![CDATA[Struggling with the second project of the Udacity course, image classification, it’s a key point to understand the process as a whole. Starting with: 1. What is a tensor?First of all, consider everything in a graph mode. TensorFlow is a programming system in which you represent computations as graphs. Nodes in the graph are called ops ( operations). An op takes zero or more Tensors, performs some computation, and produces zero or more Tensors. A Tensor is a typed multi-dimensional array. For example, you can represent a mini-batch of images as a 4-D array of floating point numbers with dimensions [batch, height, width, channels] Then we can claim: TensorFlow ~ Tensor + Flow I.e. tensors (units of array-like data) flowing through nodes (different kinds of operations, inner product, sigmoid, softmax, relu, etc), all the nodes forms a neural network graph. Demo of a small NN. 2. What is a session?12$ sess = tf.Session()$ sess.run(x, feed_dict=&#123;~&#125;) It’s actually bringing the graph framework into implementation. On the other hand, like the lazy computation of Spark RDDs, tf.placeholder() simply allocates a block of memory for future use in sess.run(), thus the computation graph/pipeline can be built ahead of any real data flow. Example of some commands that are part of the graph: 123$ input = tf.placeholder(tf.float32, (None, h, w, d))$ filter_weights = tf.Variable(tf.truncated_normal((H, W, in_d, out_d)))$ bias = tf.Variable(tf.zeros(conv_num_outputs)) 12345$ conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')$ conv_layer = tf.nn.bias_add(conv_layer, bias)$ conv_layer = tf.nn.relu(conv_layer)$ conv_layer = tf.nn.max_pool( conv_layer, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')$ session.run(optimizer, feed_dict=&#123;x: feature_batch, y: label_batch, keep_prob: keep_probability&#125;) 3. Why CNN? Regular neural nets: nodes in a single layer are independent, will generate huge scale of weights. Full connectivity is a waste of “adjacent” info, number of parameters lead to overfitting. The depth of filter is just like different nodes in a single layer.(To capture different levels of info) Number of filters ~ depth of filters For the same filter (same depth), share all parameters. Example: Input shape 32x32x3 (H, W, D_Channel) 20 filters of shape 8x8x3, stride 2, padding 1. Output shape -&gt; 14x14x20 No P sharing: (8x8x3+1)x(14x14x20) With P sharing: (8x8x3+1)x20]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SQL queries review]]></title>
      <url>%2F2017%2F03%2F08%2FSQL_query%2F</url>
      <content type="text"><![CDATA[Concept Review What does (relational) DB bring? It’s actually a collection of info organized to afford efficient retrieval. DBMS: software packages designed to store, access, manage DBs. In relational DB: Every thing is table. Under the basic structure, relational algebra is working, closure &amp; optimize. Always keep in mind of algebra optimization when querying. | DBMS | Database | Schema | Table | Attribute || — | — | — | — | — || Property Firm | House | Floor Plan | Room | Decoration | Join Be used to apply Non-Equi-Joins 1234select e.emp_id, e.fnamefrom employee e Inner Join product pOn e.s_date&gt;= p.date_offerAnd e.s_date&lt; p.date_retire Self-Joins and use subqueries as tables AggregationBe careful about implicit/explicit groups.Example 1: group ~ product_cd = ‘CHK’1234567SELECT MAX(avail_balance) max_balance,-&gt; MIN(avail_balance) min_balance,-&gt; AVG(avail_balance) avg_balance,-&gt; SUM(avail_balance) tot_balance,-&gt; COUNT(*) num_accounts-&gt; FROM account-&gt; WHERE product_cd = 'CHK'; Example 2: group ~ product_cd12345678SELECT product_cd,-&gt; MAX(avail_balance) max_balance,-&gt; MIN(avail_balance) min_balance,-&gt; AVG(avail_balance) avg_balance,-&gt; SUM(avail_balance) tot_balance,-&gt; COUNT(*) num_accts-&gt; FROM account-&gt; GROUP BY product_cd; Example 3: Multicolumn Grouping, understanding: what being grouped on will perform as “P Key” on the resulting table.1234SELECT product_cd, open_branch_id,-&gt; SUM(avail_balance) tot_balance-&gt; FROM account-&gt; GROUP BY product_cd, open_branch_id; Example 4: Expressions12SELECT MAX(pending_balance - avail_balance) max_uncleared-&gt; FROM account; 1234SELECT EXTRACT(YEAR FROM start_date) year,-&gt; COUNT(*) how_many-&gt; FROM employee-&gt; GROUP BY EXTRACT(YEAR FROM start_date); String manipulation (RE)Keyword: Like| Search Expression | Interpretation || — | — || F% | Strings beginning with F || %t | Strings ending with t || %bas% | Strings containing the substring ‘bas’ || __t_ | Four-character strings with a t in the third position | Keyword: REGEXPE.g. Find all employees whose last name starts with F or G.123SELECT emp_id, fname, lnameFROM employeeWHERE lname REGEXP '^[FG]'; Mistakes collection when Writing queries If you want to generate a column/set, must select the data from table in the first place.Demo.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Some Thinking on Tree Based Model]]></title>
      <url>%2F2017%2F03%2F08%2FTree_Model_Thinking%2F</url>
      <content type="text"><![CDATA[Question1: How does ensemble of different tree-based models enhance the performance?Question2: For important but also highly correlated features, how do they affect performance?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Introduction]]></title>
      <url>%2F2017%2F03%2F05%2FFirst-post%2F</url>
      <content type="text"><![CDATA[This is a personal tech blog. On the way of becoming a data scientist. To properly use Markdown.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Get Started With HEXO]]></title>
      <url>%2F2017%2F03%2F05%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
  
  
</search>
